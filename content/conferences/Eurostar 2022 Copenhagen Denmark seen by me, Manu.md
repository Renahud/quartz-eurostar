---
title: "Eurostar 2022 Copenhagen Denmark seen by me, Manu"
tags: [Manu]
---
[[people/Manu/Emmanuel]]

# Eurostar 2022 Copenhagen Denmark seen by me, Manu

**_EuroSTAR_** _stands for_ **_Euro_**_pean_ **_S_**_oftware_ **_T_**_esting_ **_A_**_nalysis &_ **_R_**_eview._

Hi !,

What the heck would a ‚Äúdata guy‚Äù from the BI-team, go to a Software Testing Conference!? 

Ps: I was accompanied by 4 Sfpd, ‚Äústrangers‚Äù to me at first, but 4 days living together changed that! We rented a house at walk distance from the conference instead of having all our separate hotel room. [The house](https://abnb.me/Lbppe6yKNqb)

[Current link to the conference, 2023, in Antwerp, Belgium!](https://conference.eurostarsoftwaretesting.com/)
  
I was a bit reluctant and skeptic but finally very pleased having had the privilege to assist at this wonderful community event.

I will try to share this excitement event by writing and drawing down some statements that I captured from the conference speakers that triggered my attention.

If you want, I am pleased to discuss these with you at any time.

  
## Tut B - Developing Critical Thinking Skills for (Testers, ‚Ä¶) (Andrew Brown)

We are all skilled in our expertise domain but how do we to think critically about what and how we are doing our job. We are vulnerable to **Cognitive Biases** and **Thinking Traps** that can catch out even the most seasoned person.

One of the most common thinking traps we face is that we tend to think in ways that confirm our existing beliefs, rather than challenging those beliefs.

Another thinking trap is that we often spend too much time collecting additional data about a problem, when we would be better off using that time to generate alternate theories about the problem‚Äôs cause.

We are vulnerable to EINSTELLUNG, or functional fixedness, where our minds solve problems using fixed patterns.

Einstellung is the development of a mechanised state of mind. Often called a problem solving set, Einstellung refers to a person's predisposition to solve a given problem in a specific manner even though better or more appropriate methods of solving the problem exist. [Wikipedia](https://en.m.wikipedia.org/wiki/Einstellung_effect)

Interesting Lectures / Videos:
-   [wiki: Confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias)
-   [http: What is a cognitive bias](https://www.verywellmind.com/what-is-a-cognitive-bias-2794963) 
-   **[video:The Most Common Cognitive Bias](https://www.youtube.com/watch?v=vKA4w2O61Xo)**
-   [video: Confirmation bias is a tendency to look for, interpret, and recall information in ways that affirm our preconception. Whenever we encounter objective facts on an issue we look at them through the lens of our own beliefs. As a result, we see and overrate where the two intercept. The bias is strongest for emotionally charged issues or when we search for desired outcomes. This joke illustrates it quite well...](https://www.youtube.com/watch?v=Kho5KvPBDSw) 
-   [video: The Confirmation Bias](https://www.youtube.com/watch?v=Kho5KvPBDSw)
- ...google it üòá

## Get out of your role...
Tester should get out of their ‚Äútester only‚Äù role, try to put your nose to your neighbors roles.
You should not find a way to go faster you should do it better.
Customer is always right => no => feedback of all intervenants ‚Ä¶ and yourself.
Just make things go smoother to take away these boring tasks to keep your mental health sanely.
Agile ‚Ä¶ ok, soft boundaries are needed no free all no constraints chaos.
Put in cv: Not certified in ‚Ä¶. And put your list üòá

## Drawing for Effective Testing
‚ÄúA picture speaks 1000 words‚Äù to quote a proverb. I rephrase this to ‚ÄúA drawing replaces 100 (textual) user stories‚Äù.
When generating tests it is sometimes better to focus on negative tests i.e., verifying the "not" designs rules (what the system is not expected to do) rather than positive tests i.e. verifying the predifined design rules.
Graphical representations help identifying tests which is difficult to extract from (textual) user stories, especially negative test cases also known as "Error Path Testing"¬†or "Failure Testing". These tests are generally harder to set up and a graph representation of the application rules helps this setup.

## The 7Ps rule
"**P**roper **P**lanning & **P**reparation, **P**revents **P**‚Ä¶ **P**oor **P**erformance"

![[7ps A well-known fact.jpeg]]

What happens before a backlog is created, and how we can make sure to validate the solution upfront, for instance, by using pretotyping, personas, RAT (Riskiest Assumption Testing), Design Sprints and much more, before we start building it.
‚ÄúSeven Ps aren‚Äôt just useful for the big things: they are useful in everyday operations and can make your job much easier.
As is, many companies don‚Äôt validate the solution upfront; they don‚Äôt know if it‚Äôs the right product for the right people, and they are too busy delivering to make sure what they do is the right thing.

- **Prototyping**, testers should be included early in the loop, they can help finding out they it is what end users and stakeholders wants‚Ä¶
- **Protoplanning** , very early everybody needs to know what will be the deliverable and all participants in the loop should be involved!!! 
  Testers can come with valuable input in the beginning of a project and help prepare a good plan.
- **Early collaboration** , in an agile organisation this should not be a problem

[The 7 Ps is a British Army adage for **Proper Planning and Preparation Prevents Piss Poor Performance**. (Millitary adage)](https://military-history.fandom.com/wiki/7_Ps_(military_adage))
[14 common reasons why projects fail, web link](https://www.forbes.com/sites/forbestechcouncil/2020/03/31/14-common-reasons-software-projects-fail-and-how-to-avoid-them/)
[14 common reasons why projects fail, local PDF](obsidian://open?vault=content&file=_attachments%2F14%20Common%20Reasons%20Software%20Projects%20Fail%20(And%20How%20To%20Avoid%20Them).pdf)

...

## The Testing Manifesto [link](https://www.growingagile.co.za/2015/04/the-testing-manifesto/)

About 2 years ago we created our version of a testing manifesto, as a quick summary of the mindset you should adopt when thinking about agile testing. We thought it was pretty cool üôÇ Apparently so did many others, and the slide we had has been retweeted and added to many presentations since then. We recently redid the slide to be a bit nicer and more visually appealing:

![[_attachments/TestingManifesto.jpg]]

## The Data Bias
Micha√´l Pilaeten CTG, Belgium

Gernder bias.
Data bias reinforced by AI and ML

[Explaining Bias in Your Data](https://blog.dataiku.com/explaining-bias-in-your-data) [local PDF](obsidian://open?vault=content&file=_attachments%2FExplaining%20Bias%20in%20Your%20Data.pdf)
[8 types of bias in data analysis and how to avoid them](https://www.techtarget.com/searchbusinessanalytics/feature/8-types-of-bias-in-data-analysis-and-how-to-avoid-them)[local PDF](obsidian://open?vault=content&file=_attachments%2F8%20types%20of%20bias%20in%20data%20analysis%20and%20how%20to%20avoid%20them%201.pdf) 
[The 6 most common types of bias when working with data](https://www.metabase.com/blog/6-most-common-type-of-data-bias-in-data-analysis)[local PDF](obsidian://open?vault=content&file=_attachments%2FThe%206%20most%20common%20types%20of%20bias%20when%20working%20with%20data.pdf)
[COGNITIVE BIASES](https://www.acaps.org/sites/acaps/files/resources/files/acaps_technical_brief_cognitive_biases_march_2016.pdf)




## Conclusion
one day... ü§Ø
